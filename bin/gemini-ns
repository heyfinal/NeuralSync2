#!/usr/bin/env python3
"""
gemini-ns: Gemini CLI + NeuralSync Integration Wrapper
Auto-launch, shared memory, and inter-agent communication
"""

import asyncio
import os
import sys
import json
import time
import subprocess
import signal
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import tempfile
import shutil

# Add NeuralSync to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from neuralsync.daemon_manager import ensure_neuralsync_running, get_daemon_manager
from neuralsync.ultra_comm import CliCommunicator
from neuralsync.config import load_config

# Configure logging
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class GeminiNSWrapper:
    """Enhanced Gemini CLI wrapper with NeuralSync integration"""
    
    def __init__(self):
        self.cli_name = "gemini-cli"
        self.capabilities = {
            "reasoning", "analysis", "research", "summarization", 
            "explanation", "creative-writing", "problem-solving",
            "multimodal-analysis", "data-interpretation"
        }
        
        self.ns_config = load_config()
        self.communicator: Optional[CliCommunicator] = None
        self.temp_files = []
        self._last_status_len = 0

    def _status(self, msg: str, end: bool = False):
        try:
            line = f"\r{msg}"
            pad = max(0, self._last_status_len - len(msg))
            sys.stderr.write(line + (" " * pad))
            sys.stderr.flush()
            self._last_status_len = len(msg)
            if end:
                sys.stderr.write("\n")
                sys.stderr.flush()
                self._last_status_len = 0
        except Exception:
            print(msg, file=sys.stderr)
        
    async def ensure_services_running(self) -> bool:
        """Ensure NeuralSync services are running"""
        try:
            # Health probe to avoid relaunch if already healthy
            try:
                import requests
                resp = requests.get(
                    f"http://{self.ns_config.bind_host}:{self.ns_config.bind_port}/health",
                    timeout=2
                )
                if resp.status_code == 200:
                    return True
            except Exception:
                pass
            return await ensure_neuralsync_running()
        except Exception as e:
            logger.error(f"Failed to ensure NeuralSync services: {e}")
            return False
            
    async def setup_communication(self) -> bool:
        """Setup inter-CLI communication"""
        try:
            self.communicator = CliCommunicator(self.cli_name, self.capabilities)
            
            # Register message handlers for inter-agent communication
            self.communicator.register_message_handler("analyze_data", self._handle_analysis_request)
            self.communicator.register_message_handler("research_topic", self._handle_research_request)
            self.communicator.register_message_handler("summarize_content", self._handle_summary_request)
            self.communicator.register_message_handler("explain_concept", self._handle_explanation_request)
            self.communicator.register_message_handler("solve_problem", self._handle_problem_request)
            self.communicator.register_message_handler("generate_content", self._handle_content_request)
            self.communicator.register_message_handler("spawn_agent", self._handle_spawn_request)
            self.communicator.register_message_handler("sync_memory", self._handle_memory_sync)
            
            await self.communicator.connect()
            logger.info("Gemini-NS communication system initialized")
            return True
            
        except Exception as e:
            logger.error(f"Failed to setup communication: {e}")
            return False
            
    async def _handle_analysis_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle data analysis request from other agents"""
        try:
            data_content = data.get('data', '')
            analysis_type = data.get('type', 'general')
            context = data.get('context', '')
            
            # Store analysis request
            await self._remember_interaction("data_analysis", f"Analyzing data ({analysis_type}): {data_content[:100]}", context)
            
            # Create analysis result
            analysis_result = f"""Data Analysis ({analysis_type}):

Data: {data_content}
Context: {context}

Analysis:
- Data structure: {type(data_content).__name__}
- Key patterns: [AI would identify patterns here]
- Insights: [AI would provide insights here]
- Recommendations: [AI would suggest actions here]
"""

            # Save to temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
                f.write(analysis_result)
                temp_file = f.name
                self.temp_files.append(temp_file)
            
            return {
                "status": "analyzed",
                "analysis_type": analysis_type,
                "output_file": temp_file,
                "insights": ["Pattern 1", "Pattern 2", "Pattern 3"],
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"Analysis request error: {e}")
            return {"status": "error", "message": str(e)}
            
    async def _handle_research_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle research request"""
        try:
            topic = data.get('topic', '')
            depth = data.get('depth', 'medium')
            context = data.get('context', '')
            
            await self._remember_interaction("research", f"Researching topic ({depth}): {topic}", context)
            
            # Generate research outline
            research_content = f"""Research Report: {topic}

Depth: {depth}
Context: {context}

## Executive Summary
[AI would provide summary here]

## Key Findings
1. [Finding 1]
2. [Finding 2]  
3. [Finding 3]

## Detailed Analysis
[AI would provide detailed analysis here]

## Sources and References
[AI would list relevant sources here]

## Conclusions
[AI would provide conclusions here]
"""

            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
                f.write(research_content)
                temp_file = f.name
                self.temp_files.append(temp_file)
            
            return {
                "status": "researched",
                "topic": topic,
                "depth": depth,
                "output_file": temp_file,
                "key_findings": ["Finding 1", "Finding 2", "Finding 3"],
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"Research request error: {e}")
            return {"status": "error", "message": str(e)}
            
    async def _handle_summary_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle content summarization request"""
        try:
            content = data.get('content', '')
            summary_type = data.get('type', 'brief')
            max_length = data.get('max_length', 500)
            
            await self._remember_interaction("summarization", f"Summarizing content ({summary_type}, max {max_length} chars)", content[:200])
            
            # Generate summary
            summary = f"""Summary ({summary_type}):

Original content length: {len(content)} characters
Target summary length: {max_length} characters

Summary:
[AI would provide intelligent summary of the content here, respecting the max_length constraint]

Key points:
- [Key point 1]
- [Key point 2]
- [Key point 3]
"""

            return {
                "status": "summarized",
                "summary_type": summary_type,
                "summary": summary,
                "original_length": len(content),
                "summary_length": len(summary),
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _handle_explanation_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle concept explanation request"""
        try:
            concept = data.get('concept', '')
            audience_level = data.get('level', 'intermediate')
            context = data.get('context', '')
            
            await self._remember_interaction("explanation", f"Explaining concept ({audience_level}): {concept}", context)
            
            # Generate explanation
            explanation = f"""Explanation: {concept}

Audience Level: {audience_level}
Context: {context}

## Simple Explanation
[AI would provide simple, clear explanation here]

## Key Components
1. [Component 1]
2. [Component 2]
3. [Component 3]

## Examples
[AI would provide relevant examples here]

## Common Misconceptions
[AI would address common misunderstandings here]

## Further Reading
[AI would suggest additional resources here]
"""

            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
                f.write(explanation)
                temp_file = f.name
                self.temp_files.append(temp_file)
            
            return {
                "status": "explained",
                "concept": concept,
                "audience_level": audience_level,
                "output_file": temp_file,
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _handle_problem_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle problem-solving request"""
        try:
            problem = data.get('problem', '')
            problem_type = data.get('type', 'general')
            constraints = data.get('constraints', [])
            
            await self._remember_interaction("problem_solving", f"Solving {problem_type} problem: {problem}", str(constraints))
            
            # Generate solution approach
            solution = f"""Problem Analysis: {problem}

Problem Type: {problem_type}
Constraints: {', '.join(constraints) if constraints else 'None specified'}

## Problem Breakdown
[AI would break down the problem into components]

## Solution Approaches
1. **Approach 1**: [Description]
   - Pros: [List advantages]
   - Cons: [List disadvantages]

2. **Approach 2**: [Description]
   - Pros: [List advantages]
   - Cons: [List disadvantages]

## Recommended Solution
[AI would provide recommended approach with reasoning]

## Implementation Steps
1. [Step 1]
2. [Step 2]
3. [Step 3]

## Risk Analysis
[AI would identify potential risks and mitigation strategies]
"""

            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
                f.write(solution)
                temp_file = f.name
                self.temp_files.append(temp_file)
            
            return {
                "status": "solved",
                "problem": problem,
                "problem_type": problem_type,
                "output_file": temp_file,
                "approaches": ["Approach 1", "Approach 2"],
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _handle_content_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle content generation request"""
        try:
            content_type = data.get('type', 'article')
            topic = data.get('topic', '')
            style = data.get('style', 'professional')
            length = data.get('length', 'medium')
            
            await self._remember_interaction("content_generation", f"Generating {content_type} about {topic} ({style}, {length})", "")
            
            # Generate content
            content = f"""# {topic}

Style: {style}
Length: {length}
Type: {content_type}

## Introduction
[AI would generate appropriate introduction here]

## Main Content
[AI would generate main content based on type, style, and length requirements]

## Conclusion
[AI would generate appropriate conclusion here]

---
Generated by Gemini-NS on {time.strftime('%Y-%m-%d %H:%M:%S')}
"""

            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
                f.write(content)
                temp_file = f.name
                self.temp_files.append(temp_file)
            
            return {
                "status": "generated",
                "content_type": content_type,
                "topic": topic,
                "style": style,
                "length": length,
                "output_file": temp_file,
                "agent": "gemini-cli",
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _handle_spawn_request(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle request to spawn another agent"""
        try:
            agent_type = data.get('agent', '')
            task = data.get('task', '')
            context = data.get('context', {})
            
            agent_commands = {
                'claude': 'claude-ns',
                'codex': 'codex-ns',
                'gemini': 'gemini-ns'
            }
            
            if agent_type not in agent_commands:
                return {"status": "error", "message": f"Unknown agent type: {agent_type}"}
                
            cmd = [agent_commands[agent_type], '--task', json.dumps({"task": task, "context": context})]
            
            try:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                await self._remember_interaction("agent_spawn", f"Spawned {agent_type} agent", task)
                
                return {
                    "status": "spawned",
                    "agent": agent_type,
                    "pid": process.pid,
                    "task": task,
                    "timestamp": time.time()
                }
                
            except Exception as e:
                return {"status": "error", "message": f"Failed to spawn {agent_type}: {e}"}
                
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _handle_memory_sync(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle memory synchronization between agents"""
        try:
            sync_type = data.get('type', 'pull')
            query = data.get('query', '')
            
            if sync_type == 'pull':
                await self._remember_interaction("memory_sync", f"Memory pull request: {query}", "")
                return {
                    "status": "synced",
                    "type": "pull", 
                    "query": query,
                    "timestamp": time.time()
                }
                
            elif sync_type == 'push':
                memory_data = data.get('memory', {})
                await self._remember_interaction("memory_push", "Received memory from another agent", str(memory_data)[:200])
                return {
                    "status": "synced",
                    "type": "push",
                    "timestamp": time.time()
                }
                
        except Exception as e:
            return {"status": "error", "message": str(e)}
            
    async def _remember_interaction(self, kind: str, text: str, context: str):
        """Store interaction in NeuralSync memory"""
        try:
            import requests
            
            headers = {}
            if self.ns_config.token:
                headers["Authorization"] = f"Bearer {self.ns_config.token}"
                
            payload = {
                "text": text,
                "kind": kind,
                "scope": "inter-agent",
                "tool": self.cli_name,
                "tags": ["auto-generated", "inter-agent", "gemini"],
                "confidence": 0.9,
                "source": "gemini-ns",
                "meta": {"context": context, "timestamp": time.time()}
            }
            
            requests.post(
                f"http://{self.ns_config.bind_host}:{self.ns_config.bind_port}/remember",
                headers=headers,
                json=payload,
                timeout=5
            )
            
        except Exception as e:
            logger.debug(f"Failed to store memory: {e}")
            
    async def get_shared_context(self) -> str:
        """Get shared persona and memory context from NeuralSync"""
        try:
            import requests
            import sqlite3
            from pathlib import Path
            
            headers = {}
            if self.ns_config.token:
                headers["Authorization"] = f"Bearer {self.ns_config.token}"
            
            # Persona via HTTP (best-effort)
            persona = ""
            try:
                resp = requests.get(
                    f"http://{self.ns_config.bind_host}:{self.ns_config.bind_port}/persona",
                    headers=headers,
                    timeout=5
                )
                if resp.status_code == 200:
                    persona = resp.json().get("text", "")
            except Exception:
                pass
            
            # Memory via HTTP recall (best-effort)
            memory_context = ""
            try:
                recall_resp = requests.post(
                    f"http://{self.ns_config.bind_host}:{self.ns_config.bind_port}/recall",
                    headers=headers,
                    json={
                        "query": "analysis research reasoning problem solving",
                        "top_k": 8,
                        "scope": "any",
                        "tool": None
                    },
                    timeout=10
                )
                if recall_resp.status_code == 200:
                    memory_context = recall_resp.json().get("preamble", "")
            except Exception:
                pass
            
            # Local DB fallback if needed
            if not persona or not memory_context:
                try:
                    db_path = Path.home()/'.neuralsync'/'memory.db'
                    if db_path.exists():
                        con = sqlite3.connect(str(db_path))
                        con.row_factory = sqlite3.Row
                        rows = con.execute(
                            """
                            SELECT kind, text, tool, source, created_at
                            FROM items
                            WHERE tombstone = 0 AND (confidence IS NULL OR confidence >= 0.6)
                            ORDER BY created_at DESC
                            LIMIT 12
                            """
                        ).fetchall()
                        if rows and not memory_context:
                            excerpts = []
                            for r in rows:
                                t = (r['text'] or '').strip()
                                if t:
                                    excerpts.append(f"- [{r['tool'] or r['source']}] {t[:200]}")
                            if excerpts:
                                memory_context = "Recent shared context:\n" + "\n".join(excerpts)
                        con.close()
                except Exception:
                    pass
            
            # Combine context
            parts = []
            if persona:
                parts.append(f"Persona: {persona}")
            if memory_context:
                parts.append(memory_context)
            return "\n\n".join(parts) + ("\n\n" if parts else "")
        except Exception as e:
            logger.debug(f"Failed to get shared context: {e}")
            return ""
            return ""
            
    async def run_gemini_cli(self, args: List[str]) -> int:
        """Run Gemini CLI with NeuralSync context injection"""
        try:
            # Get shared context
            shared_context = await self.get_shared_context()
            
            # Find gemini CLI executable
            gemini_cmd = shutil.which('gemini')
            if not gemini_cmd:
                print("ERROR: gemini CLI not found in PATH", file=sys.stderr)
                print("Please install Gemini CLI first", file=sys.stderr)
                return 1
                
            # Prepare environment
            env = os.environ.copy()
            env.update({
                'NS_HOST': self.ns_config.bind_host,
                'NS_PORT': str(self.ns_config.bind_port),
                'NS_TOKEN': self.ns_config.token,
                'NEURALSYNC_CONTEXT': shared_context,
                'CLI_WRAPPER': 'gemini-ns'
            })
            
            # Handle stdin input with context enhancement (supports repair override)
            repair_override = os.environ.get('NEURALSYNC_REPAIR_INPUT')
            if repair_override is not None or (not args or (len(args) == 1 and args[0] in ['-', '--stdin'])):
                input_data = repair_override if repair_override is not None else (sys.stdin.read() if not sys.stdin.isatty() else "")
                
                # Enhance input with context for better reasoning
                enhanced_input = f"""{shared_context}

Reasoning/Analysis Request:
{input_data}

Please provide thoughtful analysis with clear reasoning, considering all available context."""

                process = subprocess.Popen(
                    [gemini_cmd],
                    stdin=subprocess.PIPE,
                    stdout=sys.stdout,
                    stderr=sys.stderr,
                    env=env,
                    text=True
                )
                
                process.communicate(input=enhanced_input)
                return process.returncode
                
            else:
                # Run with args and enhanced environment
                process = subprocess.run(
                    [gemini_cmd] + args,
                    env=env
                )
                return process.returncode
                
        except Exception as e:
            logger.error(f"Failed to run gemini CLI: {e}")
            print(f"ERROR: {e}", file=sys.stderr)
            return 1
            
    def cleanup(self):
        """Cleanup temporary files"""
        for temp_file in self.temp_files:
            try:
                os.unlink(temp_file)
            except:
                pass
                
    async def run(self, args: List[str]) -> int:
        """Main run method"""
        try:
            # Handle special arguments
            if len(args) > 0:
                if args[0] == '--neuralsync-status':
                    manager = get_daemon_manager()
                    status = manager.get_system_info()
                    print(json.dumps(status, indent=2))
                    return 0
                    
                elif args[0] == '--spawn-agent':
                    if len(args) < 3:
                        print("Usage: gemini-ns --spawn-agent <agent_type> <task>", file=sys.stderr)
                        return 1
                        
                    agent_type = args[1]
                    task = ' '.join(args[2:])
                    
                    if self.communicator:
                        result = await self._handle_spawn_request({
                            'agent': agent_type,
                            'task': task,
                            'context': {}
                        })
                        print(json.dumps(result, indent=2))
                        return 0 if result['status'] != 'error' else 1
                        
                elif args[0] == '--task':
                    # Handle spawned task
                    if len(args) > 1:
                        try:
                            task_data = json.loads(args[1])
                            task = task_data.get('task', '')
                            context = task_data.get('context', {})
                            
                            await self._remember_interaction("spawned_task", f"Processing spawned task: {task}", str(context))
                            
                            # Create task input for gemini CLI
                            task_input = f"""Task: {task}
Context: {json.dumps(context, indent=2)}

Please analyze this task and provide comprehensive insights and recommendations."""
                            
                            return await self.run_gemini_cli([])
                            
                        except json.JSONDecodeError:
                            print("ERROR: Invalid task JSON", file=sys.stderr)
                            return 1
            
            # Ensure NeuralSync services are running (single-line status)
            self._status("NS: checking services…")
            repair_msg = ""
            services_ready = await self.ensure_services_running()
            if services_ready:
                self._status("NS: ready · linking comms…")
                ok = await self.setup_communication()
                if ok:
                    self._status("NS: ready", end=True)
                else:
                    self._status("NS: comms failed · self-repair…", end=True)
                    repair_msg = "NeuralSync communication setup failed. Please analyze and fix NeuralSync v2 server and wrappers so that /health, /persona, /recall work on 127.0.0.1:8373."
            else:
                self._status("NS: unavailable · self-repair…", end=True)
                repair_msg = "NeuralSync services unavailable. Please restore NeuralSync v2 so wrappers can fetch persona/recall."

            if repair_msg:
                os.environ['NEURALSYNC_REPAIR_INPUT'] = (
                    f"SYSTEM SELF-REPAIR MODE (macOS):\n"
                    f"Issue: {repair_msg}\n"
                    f"Constraints: minimal, focused patches; do not break APIs.\n"
                    f"Targets: ~/NeuralSync2/neuralsync (enhanced_server.py, storage.py, research_dedup.py, daemon_manager.py).\n"
                    f"Goals:\n- Ensure server binds to 127.0.0.1:8373 and exposes /persona,/remember,/recall.\n- Add SQLite busy_timeout and thread-safety.\n- Fix mmap resize safety.\n- Make heavy deps optional.\n- Restart server and verify /health.\n"
                    f"Deliver: file diffs and restart steps; then verify with curl.\n"
                )
                self._status("NS: self-repair initiated via gemini", end=True)
                
            # Run gemini CLI with NeuralSync integration
            rc = await self.run_gemini_cli(args)
            if 'NEURALSYNC_REPAIR_INPUT' in os.environ:
                os.environ.pop('NEURALSYNC_REPAIR_INPUT', None)
            return rc
            
        finally:
            # Cleanup
            self.cleanup()
            if self.communicator:
                await self.communicator.disconnect()


async def main():
    """Main entry point"""
    wrapper = GeminiNSWrapper()
    
    # Handle signals gracefully
    def signal_handler(sig, frame):
        asyncio.create_task(wrapper.communicator.disconnect() if wrapper.communicator else asyncio.sleep(0))
        sys.exit(0)
        
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Run wrapper
    try:
        return_code = await wrapper.run(sys.argv[1:])
        sys.exit(return_code)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        logger.error(f"Gemini-NS wrapper error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
