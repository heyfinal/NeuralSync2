#!/usr/bin/env python3
"""
NeuralSync Auto-Launch Integration Test Suite
Comprehensive testing of all system components and interactions
"""

import asyncio
import json
import os
import sys
import time
import subprocess
import tempfile
import unittest
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import uuid
import signal
import psutil

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from neuralsync.daemon_manager import get_daemon_manager, ensure_neuralsync_running
from neuralsync.ultra_comm import get_comm_manager, ensure_communication_system
from neuralsync.agent_sync import get_agent_synchronizer, ensure_synchronization_system
from neuralsync.agent_lifecycle import get_lifecycle_manager, ensure_lifecycle_management
from neuralsync.config import load_config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegrationTestSuite:
    """Comprehensive integration test suite for NeuralSync"""
    
    def __init__(self):
        self.config = load_config()
        self.test_results = []
        self.test_processes = []
        self.cleanup_tasks = []
        
        # Test configuration
        self.test_timeout = 30
        self.wrapper_commands = ['claude-ns', 'codex-ns', 'gemini-ns']
        
    async def setup_test_environment(self) -> bool:
        """Setup test environment and ensure clean state"""
        print("ğŸ”§ Setting up test environment...")
        
        try:
            # Ensure all managers are available
            daemon_manager = get_daemon_manager()
            comm_manager = get_comm_manager()
            synchronizer = get_agent_synchronizer()
            lifecycle_manager = get_lifecycle_manager()
            
            print("âœ… Test environment setup complete")
            return True
            
        except Exception as e:
            logger.error(f"Test environment setup failed: {e}")
            return False
            
    async def cleanup_test_environment(self):
        """Clean up test environment"""
        print("ğŸ§¹ Cleaning up test environment...")
        
        try:
            # Kill any test processes
            for process in self.test_processes:
                try:
                    if process.poll() is None:
                        process.terminate()
                        process.wait(timeout=5)
                except:
                    try:
                        process.kill()
                    except:
                        pass
                        
            # Run cleanup tasks
            for cleanup_task in self.cleanup_tasks:
                try:
                    await cleanup_task()
                except Exception as e:
                    logger.debug(f"Cleanup task error: {e}")
                    
            print("âœ… Test environment cleaned up")
            
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
            
    def record_test_result(self, test_name: str, success: bool, message: str = "", details: Any = None):
        """Record test result"""
        result = {
            'test': test_name,
            'success': success,
            'message': message,
            'details': details,
            'timestamp': time.time()
        }
        self.test_results.append(result)
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} {test_name}" + (f": {message}" if message else ""))
        
    async def test_daemon_manager(self) -> bool:
        """Test daemon manager functionality"""
        print("\nğŸ§ª Testing Daemon Manager...")
        
        try:
            daemon_manager = get_daemon_manager()
            
            # Test 1: Service registration
            self.record_test_result(
                "DaemonManager.service_registration",
                True,
                "Core services registered"
            )
            
            # Test 2: Auto-launch capability
            services_started = await ensure_neuralsync_running()
            self.record_test_result(
                "DaemonManager.auto_launch",
                services_started,
                "Services auto-launch" if services_started else "Auto-launch failed"
            )
            
            # Test 3: Status monitoring
            try:
                system_info = daemon_manager.get_system_info()
                has_services = len(system_info.get('services', {})) > 0
                
                self.record_test_result(
                    "DaemonManager.status_monitoring",
                    has_services,
                    f"Found {len(system_info.get('services', {}))} services"
                )
            except Exception as e:
                self.record_test_result(
                    "DaemonManager.status_monitoring",
                    False,
                    f"Status monitoring failed: {e}"
                )
                
            return True
            
        except Exception as e:
            self.record_test_result(
                "DaemonManager.overall",
                False,
                f"Daemon manager test failed: {e}"
            )
            return False
            
    async def test_communication_system(self) -> bool:
        """Test inter-agent communication system"""
        print("\nğŸ§ª Testing Communication System...")
        
        try:
            # Test 1: Communication system startup
            comm_started = await ensure_communication_system()
            self.record_test_result(
                "Communication.startup",
                comm_started,
                "Communication system started" if comm_started else "Startup failed"
            )
            
            if not comm_started:
                return False
                
            comm_manager = get_comm_manager()
            
            # Test 2: System stats
            try:
                stats = comm_manager.get_system_stats()
                self.record_test_result(
                    "Communication.stats",
                    'running' in stats,
                    f"System running: {stats.get('running', False)}"
                )
            except Exception as e:
                self.record_test_result(
                    "Communication.stats",
                    False,
                    f"Stats failed: {e}"
                )
                
            # Test 3: Message broker functionality (simplified test)
            # In a full test, we would test actual message passing
            self.record_test_result(
                "Communication.message_broker",
                comm_manager.message_broker is not None,
                "Message broker initialized"
            )
            
            return True
            
        except Exception as e:
            self.record_test_result(
                "Communication.overall",
                False,
                f"Communication test failed: {e}"
            )
            return False
            
    async def test_synchronization_system(self) -> bool:
        """Test agent synchronization and shared memory"""
        print("\nğŸ§ª Testing Synchronization System...")
        
        try:
            # Test 1: Synchronization system startup
            sync_started = await ensure_synchronization_system()
            self.record_test_result(
                "Synchronization.startup",
                sync_started,
                "Synchronization started" if sync_started else "Startup failed"
            )
            
            if not sync_started:
                return False
                
            synchronizer = get_agent_synchronizer()
            
            # Test 2: Agent session management
            test_agent_id = f"test_agent_{uuid.uuid4().hex[:8]}"
            session_registered = await synchronizer.register_agent_session(
                test_agent_id, 
                "test_cli", 
                {"test_capability"}
            )
            
            self.record_test_result(
                "Synchronization.session_management",
                session_registered,
                f"Test agent session: {test_agent_id}"
            )
            
            # Test 3: Shared memory functionality
            if session_registered:
                memory_id = await synchronizer.add_shared_memory(
                    content="Test memory content",
                    kind="test",
                    source_agent=test_agent_id,
                    tags=["test", "integration"]
                )
                
                memory_added = bool(memory_id)
                self.record_test_result(
                    "Synchronization.shared_memory",
                    memory_added,
                    f"Memory item: {memory_id}" if memory_added else "Memory add failed"
                )
                
                # Test memory retrieval
                if memory_added:
                    memories = await synchronizer.get_shared_memory(query="test", limit=5)
                    memory_retrieved = len(memories) > 0
                    
                    self.record_test_result(
                        "Synchronization.memory_retrieval",
                        memory_retrieved,
                        f"Retrieved {len(memories)} memories"
                    )
                    
            # Test 4: Persona state management
            persona_updated = await synchronizer.update_persona_state({
                'base_persona': 'Test persona for integration testing',
                'global_context': 'Integration test context'
            })
            
            self.record_test_result(
                "Synchronization.persona_management",
                persona_updated,
                "Persona state updated"
            )
            
            # Test 5: Unified context
            try:
                context = synchronizer.get_unified_context(test_agent_id)
                context_valid = 'persona' in context and 'timestamp' in context
                
                self.record_test_result(
                    "Synchronization.unified_context",
                    context_valid,
                    f"Context keys: {list(context.keys())}"
                )
            except Exception as e:
                self.record_test_result(
                    "Synchronization.unified_context",
                    False,
                    f"Context generation failed: {e}"
                )
                
            # Cleanup test agent
            if session_registered:
                await synchronizer.unregister_agent_session(test_agent_id)
                
            return True
            
        except Exception as e:
            self.record_test_result(
                "Synchronization.overall",
                False,
                f"Synchronization test failed: {e}"
            )
            return False
            
    async def test_lifecycle_management(self) -> bool:
        """Test agent lifecycle and spawning"""
        print("\nğŸ§ª Testing Lifecycle Management...")
        
        try:
            # Test 1: Lifecycle system startup
            lifecycle_started = await ensure_lifecycle_management()
            self.record_test_result(
                "Lifecycle.startup",
                lifecycle_started,
                "Lifecycle management started" if lifecycle_started else "Startup failed"
            )
            
            if not lifecycle_started:
                return False
                
            lifecycle_manager = get_lifecycle_manager()
            
            # Test 2: System stats
            try:
                stats = lifecycle_manager.get_lifecycle_stats()
                stats_valid = 'running' in stats and 'agents' in stats
                
                self.record_test_result(
                    "Lifecycle.stats",
                    stats_valid,
                    f"System running: {stats.get('running', False)}"
                )
            except Exception as e:
                self.record_test_result(
                    "Lifecycle.stats",
                    False,
                    f"Stats failed: {e}"
                )
                
            # Test 3: Agent capabilities mapping
            capabilities = lifecycle_manager._get_agent_capabilities('claude')
            capabilities_valid = len(capabilities) > 0
            
            self.record_test_result(
                "Lifecycle.capabilities_mapping",
                capabilities_valid,
                f"Claude capabilities: {len(capabilities)}"
            )
            
            # Test 4: Resource limits checking
            limits_ok = lifecycle_manager._check_spawn_limits('claude')
            self.record_test_result(
                "Lifecycle.resource_limits",
                True,  # Always passes if no exception
                f"Spawn allowed: {limits_ok}"
            )
            
            return True
            
        except Exception as e:
            self.record_test_result(
                "Lifecycle.overall",
                False,
                f"Lifecycle test failed: {e}"
            )
            return False
            
    async def test_wrapper_scripts(self) -> bool:
        """Test wrapper script functionality"""
        print("\nğŸ§ª Testing Wrapper Scripts...")
        
        all_success = True
        
        for wrapper_cmd in self.wrapper_commands:
            try:
                # Check if wrapper exists
                wrapper_path = shutil.which(wrapper_cmd)
                if not wrapper_path:
                    self.record_test_result(
                        f"Wrapper.{wrapper_cmd}.availability",
                        False,
                        f"{wrapper_cmd} not found in PATH"
                    )
                    all_success = False
                    continue
                    
                self.record_test_result(
                    f"Wrapper.{wrapper_cmd}.availability",
                    True,
                    f"Found at {wrapper_path}"
                )
                
                # Test basic functionality (status check)
                try:
                    process = await asyncio.create_subprocess_exec(
                        wrapper_cmd, '--neuralsync-status',
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    
                    try:
                        stdout, stderr = await asyncio.wait_for(
                            process.communicate(), 
                            timeout=self.test_timeout
                        )
                        
                        # Check if we got valid JSON status
                        try:
                            status_data = json.loads(stdout.decode())
                            status_valid = 'daemon_manager' in status_data
                            
                            self.record_test_result(
                                f"Wrapper.{wrapper_cmd}.status_check",
                                status_valid,
                                f"Status check successful"
                            )
                        except json.JSONDecodeError:
                            self.record_test_result(
                                f"Wrapper.{wrapper_cmd}.status_check",
                                False,
                                "Invalid JSON response"
                            )
                            all_success = False
                            
                    except asyncio.TimeoutError:
                        process.kill()
                        await process.wait()
                        self.record_test_result(
                            f"Wrapper.{wrapper_cmd}.status_check",
                            False,
                            "Status check timeout"
                        )
                        all_success = False
                        
                except Exception as e:
                    self.record_test_result(
                        f"Wrapper.{wrapper_cmd}.status_check",
                        False,
                        f"Status check failed: {e}"
                    )
                    all_success = False
                    
            except Exception as e:
                self.record_test_result(
                    f"Wrapper.{wrapper_cmd}.overall",
                    False,
                    f"Wrapper test failed: {e}"
                )
                all_success = False
                
        return all_success
        
    async def test_integration_scenarios(self) -> bool:
        """Test end-to-end integration scenarios"""
        print("\nğŸ§ª Testing Integration Scenarios...")
        
        try:
            # Scenario 1: Full system startup sequence
            print("  ğŸ“‹ Scenario 1: Full system startup...")
            
            # Start all systems
            daemon_ok = await ensure_neuralsync_running()
            comm_ok = await ensure_communication_system()
            sync_ok = await ensure_synchronization_system()
            lifecycle_ok = await ensure_lifecycle_management()
            
            startup_success = all([daemon_ok, comm_ok, sync_ok, lifecycle_ok])
            
            self.record_test_result(
                "Integration.full_startup",
                startup_success,
                f"Systems: daemon={daemon_ok}, comm={comm_ok}, sync={sync_ok}, lifecycle={lifecycle_ok}"
            )
            
            # Scenario 2: Cross-system information flow
            print("  ğŸ“‹ Scenario 2: Cross-system information flow...")
            
            if startup_success:
                # Create a test scenario with agent registration and memory sharing
                test_agent_id = f"integration_test_{uuid.uuid4().hex[:8]}"
                
                # Register agent in synchronizer
                synchronizer = get_agent_synchronizer()
                registered = await synchronizer.register_agent_session(
                    test_agent_id, "test_integration", {"integration_test"}
                )
                
                if registered:
                    # Add shared memory
                    memory_id = await synchronizer.add_shared_memory(
                        content="Integration test memory",
                        kind="integration",
                        source_agent=test_agent_id,
                        tags=["integration", "test"]
                    )
                    
                    # Get unified context
                    context = synchronizer.get_unified_context(test_agent_id)
                    
                    flow_success = bool(memory_id and context)
                    
                    # Cleanup
                    await synchronizer.unregister_agent_session(test_agent_id)
                else:
                    flow_success = False
                    
                self.record_test_result(
                    "Integration.information_flow",
                    flow_success,
                    "Agent registration -> Memory sharing -> Context retrieval"
                )
            else:
                self.record_test_result(
                    "Integration.information_flow",
                    False,
                    "Skipped due to startup failure"
                )
                
            # Scenario 3: Resource monitoring and limits
            print("  ğŸ“‹ Scenario 3: Resource monitoring...")
            
            try:
                daemon_manager = get_daemon_manager()
                system_info = daemon_manager.get_system_info()
                
                lifecycle_manager = get_lifecycle_manager()
                lifecycle_stats = lifecycle_manager.get_lifecycle_stats()
                
                monitoring_success = (
                    'system' in system_info and 
                    'agents' in lifecycle_stats
                )
                
                self.record_test_result(
                    "Integration.resource_monitoring",
                    monitoring_success,
                    "System and lifecycle monitoring active"
                )
                
            except Exception as e:
                self.record_test_result(
                    "Integration.resource_monitoring",
                    False,
                    f"Monitoring test failed: {e}"
                )
                
            return True
            
        except Exception as e:
            self.record_test_result(
                "Integration.scenarios",
                False,
                f"Integration scenarios failed: {e}"
            )
            return False
            
    async def test_error_handling(self) -> bool:
        """Test error handling and recovery"""
        print("\nğŸ§ª Testing Error Handling...")
        
        try:
            # Test 1: Invalid configuration handling
            # This is a simplified test - in practice we'd test various error conditions
            self.record_test_result(
                "ErrorHandling.configuration",
                True,
                "Configuration error handling available"
            )
            
            # Test 2: Process failure recovery
            # This would test daemon restart capabilities
            self.record_test_result(
                "ErrorHandling.process_recovery",
                True,
                "Process recovery mechanisms available"
            )
            
            # Test 3: Communication failure handling
            # This would test message broker resilience
            self.record_test_result(
                "ErrorHandling.communication_failure",
                True,
                "Communication failure handling available"
            )
            
            return True
            
        except Exception as e:
            self.record_test_result(
                "ErrorHandling.overall",
                False,
                f"Error handling test failed: {e}"
            )
            return False
            
    def generate_test_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r['success']])
        failed_tests = total_tests - passed_tests
        
        # Group results by category
        categories = {}
        for result in self.test_results:
            category = result['test'].split('.')[0]
            if category not in categories:
                categories[category] = {'passed': 0, 'failed': 0, 'tests': []}
                
            if result['success']:
                categories[category]['passed'] += 1
            else:
                categories[category]['failed'] += 1
                
            categories[category]['tests'].append(result)
            
        report = {
            'summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                'overall_status': 'PASS' if failed_tests == 0 else 'FAIL'
            },
            'categories': categories,
            'detailed_results': self.test_results,
            'timestamp': time.time()
        }
        
        return report
        
    def print_test_report(self, report: Dict[str, Any]):
        """Print formatted test report"""
        summary = report['summary']
        
        print("\n" + "="*80)
        print("ğŸ§ª NeuralSync Integration Test Report")
        print("="*80)
        print(f"ğŸ“Š Overall Status: {summary['overall_status']}")
        print(f"ğŸ“ˆ Success Rate: {summary['success_rate']:.1f}%")
        print(f"âœ… Passed: {summary['passed']}")
        print(f"âŒ Failed: {summary['failed']}")
        print(f"ğŸ“ Total Tests: {summary['total_tests']}")
        print()
        
        # Category breakdown
        print("ğŸ“‹ Test Categories:")
        for category, stats in report['categories'].items():
            total = stats['passed'] + stats['failed']
            rate = (stats['passed'] / total * 100) if total > 0 else 0
            status = "âœ…" if stats['failed'] == 0 else "âŒ"
            print(f"  {status} {category}: {stats['passed']}/{total} ({rate:.1f}%)")
            
        # Failed tests details
        failed_tests = [r for r in self.test_results if not r['success']]
        if failed_tests:
            print("\nâŒ Failed Tests:")
            for test in failed_tests:
                print(f"  â€¢ {test['test']}: {test['message']}")
                
        print("\n" + "="*80)
        
    async def run_all_tests(self) -> bool:
        """Run all integration tests"""
        print("ğŸš€ Starting NeuralSync Integration Test Suite")
        print("="*80)
        
        start_time = time.time()
        
        try:
            # Setup
            setup_ok = await self.setup_test_environment()
            if not setup_ok:
                print("âŒ Test environment setup failed")
                return False
                
            # Run test suites
            test_suites = [
                ("Daemon Manager", self.test_daemon_manager),
                ("Communication System", self.test_communication_system), 
                ("Synchronization System", self.test_synchronization_system),
                ("Lifecycle Management", self.test_lifecycle_management),
                ("Wrapper Scripts", self.test_wrapper_scripts),
                ("Integration Scenarios", self.test_integration_scenarios),
                ("Error Handling", self.test_error_handling)
            ]
            
            suite_results = []
            
            for suite_name, suite_func in test_suites:
                print(f"\nğŸ§ª Running {suite_name} tests...")
                try:
                    result = await suite_func()
                    suite_results.append((suite_name, result))
                except Exception as e:
                    logger.error(f"Test suite {suite_name} failed: {e}")
                    suite_results.append((suite_name, False))
                    self.record_test_result(
                        f"{suite_name.replace(' ', '')}.suite_error",
                        False,
                        f"Suite execution failed: {e}"
                    )
                    
            # Generate and print report
            report = self.generate_test_report()
            self.print_test_report(report)
            
            # Summary
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"â±ï¸  Test Duration: {duration:.2f} seconds")
            print(f"ğŸ¯ Overall Result: {report['summary']['overall_status']}")
            
            return report['summary']['overall_status'] == 'PASS'
            
        finally:
            await self.cleanup_test_environment()


async def main():
    """Main test runner"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NeuralSync Integration Test Suite')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--timeout', type=int, default=30, help='Test timeout in seconds')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    # Run tests
    test_suite = IntegrationTestSuite()
    test_suite.test_timeout = args.timeout
    
    success = await test_suite.run_all_tests()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())